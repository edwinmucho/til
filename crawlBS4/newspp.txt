# -*- coding: utf-8 -*- from scrapy.spiders import Spider from scrapy import Selector from pagedaum.items import DaumcrawlerItem from scrapy.http.request import Request start_page=1 end_page=1000 class MySpider(Spider): name = "pagedaum" allowed_domains=["daum.net"] start_urls=["http://search.daum.net/search?w=web&q=site:agora.media.daum.net%20%EC%9E%A5%EB%8F%99%EA%B1%B4"] def parse(self, response): global start_page,end_page #글로벌변수 선언 alldata = Selector(response) mylist = alldata.xpath("//*[@class='wrap_cont']//div[1]") for mydata in mylist: item = DaumcrawlerItem() item['title'] = mydata.xpath("div[1]/a//text()").extract() item['content'] = mydata.xpath("p[1]//text()").extract() item['addr'] = mydata.xpath("div[1]/a/@href").extract() yield item #한 페이지에 대한 작업이 모두 완료되면 새로 요청을 해야함. #새로 요청을 하기위한 조건-임의의 변수(시작,종료페이지)를 정의하고 작업 if start_page<=3: start_page = start_page+1 #페이지번호 증가 #새로 요청할 페이지의 주소 next_page = ["http://search.daum.net/search?w=web&q=site:agora.media.daum.net%20%EC%9E%A5%EB%8F%99%EA%B1%B4&DA=PGD&p="+str(start_page)] #작업이 완료되면 새로운 요청을 실행한다. - Request yield Request(next_page[0],self.parse) #어떤 메소드를 새로 요청할것인지 / 요청 후 어떤 클래스로 넘어갈것인지.
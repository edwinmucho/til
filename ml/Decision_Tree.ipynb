{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 의사결정 트리\n",
    "* 주로 하향식 기법이 사용되며, 각 진행 단계에서는 주어진 데이터 집합을 가장 적합한 기준으로 분할하는 변수값이 선택된다.\n",
    "* 분할의 적합성을 측정하는 알고리즘에 따라 ID3, C4.5(C5.0), CART, CHAID 등의 기법이 있다.\n",
    "\n",
    "* 데이터를 어떻게 분할하고 데이터 분할을 언제 멈추어야 하는지에 대한 내용이 기본전제 되어 있다.\n",
    "\n",
    "### 장점 \n",
    " * 학습결과를 사람이 이해하기 쉽다.\n",
    " * 자료를 가공할 필요가 거의 없다.\n",
    " * 관련없는 속성이 있어도 처리 가능하다.\n",
    "\n",
    "### 단점\n",
    " * Overfitting 되기 쉽다.\n",
    " * 훈련데이터를 제대로 일반화 하지 못하면 복잡한 트리가 만들어 질수도 있다.\n",
    "---\n",
    "\n",
    "### 알고리즘 종류\n",
    "ID3 알고리즘 ( Entropy / 다지분리(범주))\n",
    " - 범주형에 주로 이용 ( Entropy 기반 수식이 범주형에만 적용하기 때문)\n",
    " - 상위 노드에 사용된 속성은 사용하지 않음.\n",
    " - 연속(수치)형 속성은 사용할 수 없음.\n",
    " - 범주 값이 많은 경우 가지 갯수가 많아짐\n",
    " - [참고 사이트](http://jihoonlee.tistory.com/16?category=725291)\n",
    "---\n",
    "C4.5 알고리즘 ( Infomation Gain / 다지분리(범주), 이진분리(수치) )\n",
    " - ID3 알고리즘의 단점을 보완한 알고리즘\n",
    " - 수치형 속성 처리 가능\n",
    " - 결측치 처리 가능\n",
    " - 속성 선택시 Branch 수에 대한 가중치 적용 가능\n",
    " - 가지치기 기능 추가\n",
    " - [참고 사이트](http://jihoonlee.tistory.com/17?category=725291)\n",
    "---\n",
    "CART 알고리즘 ( GINI Index(범주), 분산의 차이(수치) / 항상 2진 분리, 통계적 접근 방식 )\n",
    " - 전체 데이터 셋을 가지고 시작\n",
    " - 반복해서 두개의 자식노드를 생성하기 위해\n",
    "   모든 예측변수를 사용하여\n",
    "   데이터 셋의 부분 집합을 쪼갬으로써 의사결정 트리 생성\n",
    "---\n",
    "CHAID 알고리즘 ( 카이제곱(범주), F검정(수치) / 통계적 접근 방식)\n",
    " - CART 알고리즘과 흡사하나 데이터 분할 방식이 다름.\n",
    " - 변수들 간의 통계적 관계를 찾는것이 목적\n",
    "---\n",
    "#### 엔트로피 및 지니 계수 계산은 참고 사이트 부분 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID3 알고리즘 코드 구현\n",
    "\n",
    "from math import log\n",
    "\n",
    "# 데이터 셋 만드는 부분\n",
    "def createDataSet():\n",
    "    dataSet = [[1, 1, 'yes'],\n",
    "               [1, 1, 'yes'],\n",
    "               [1, 0, 'no'],\n",
    "               [0, 1, 'no'],\n",
    "               [0, 1, 'no']]\n",
    "    labels = ['no surfacing', 'flippers']\n",
    "    return dataSet, labels\n",
    "\n",
    "# Entropy 계산 하는 부분.\n",
    "def calcShannonEnt(dataSet):\n",
    "    numEntries = len(dataSet) # 전체의 데이터 갯수\n",
    "    labelCnt = {}\n",
    "    \n",
    "    # 딕셔너리 생성.\n",
    "    for featvec in dataSet:\n",
    "        curlab = featvec[-1] \n",
    "        if curlab not in labelCnt.keys():\n",
    "            labelCnt[curlab] = 0\n",
    "        labelCnt[curlab] += 1   # 키의 빈도\n",
    "        \n",
    "    shannonEnt = 0\n",
    "    for k in labelCnt.values():\n",
    "        prob = float(k) / numEntries  # 확률 계산\n",
    "        shannonEnt -= prob * log(prob, 2)  # ShanonEntropy 계산  : -시그마( 확률 * log2(확률))\n",
    "    return shannonEnt\n",
    "\n",
    "# DataSet 분할\n",
    "def splitDataSet(dataSet, axis, value):\n",
    "    retDataSet = []\n",
    "    for featVec in dataSet:\n",
    "#         print(\"{}\\n{}/{}/{}\".format(featVec,featVec[:axis], featVec[axis], featVec[axis+1:]))\n",
    "        if featVec[axis] == value:\n",
    "            reducedFeatVec = featVec[:axis]\n",
    "            reducedFeatVec.extend(featVec[axis+1:])\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "    return retDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "myData, labels = createDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['no surfacing', 'flippers']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myData\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9709505944546686"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calcShannonEnt(myData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 'yes']\n",
      "[1]/1/['yes']\n",
      "[1, 1, 'yes']\n",
      "[1]/1/['yes']\n",
      "[1, 0, 'no']\n",
      "[1]/0/['no']\n",
      "[0, 1, 'no']\n",
      "[0]/1/['no']\n",
      "[0, 1, 'no']\n",
      "[0]/1/['no']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, 'yes'], [1, 'yes'], [0, 'no'], [0, 'no']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitDataSet(myData, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적의 분할 Feature 찾기\n",
    "# 최적의 분할은 분할 전 엔트로피 - 분할 후 엔트로피 의 차이가 큰 Feature 라고 보면 됨. \n",
    "# 이것을 Infomation Gain \n",
    "def chooseBestFeatureToSplit(dataSet):\n",
    "    numFeatures = len(dataSet[0]) - 1\n",
    "    baseEntropy = calcShannonEnt(dataSet)\n",
    "#     print(\"*Base Ent : \", baseEntropy)\n",
    "    bestInfoGain = 0.0\n",
    "    bestFeature = -1\n",
    "    \n",
    "    for i in range(numFeatures):\n",
    "        featList = [ex[i] for ex in dataSet]\n",
    "#         print(featList)\n",
    "        uniqueVals = set(featList)\n",
    "#         print(\"U\",uniqueVals)\n",
    "        newEnt = 0.0\n",
    "        for val in uniqueVals:\n",
    "            subDataSet = splitDataSet(dataSet, i, val)\n",
    "#             print(subDataSet)\n",
    "            prob = len(subDataSet)/float(len(dataSet))\n",
    "#             print(\"Prob\",prob)\n",
    "            newEnt += prob * calcShannonEnt(subDataSet)\n",
    "#             print(\"NewEnt : \",newEnt)\n",
    "        infoGain = baseEntropy - newEnt\n",
    "#         print(\"* infoG: \",infoGain)\n",
    "        if (infoGain > bestInfoGain):\n",
    "            bestInfoGain = infoGain\n",
    "            bestFeature = i\n",
    "    return bestFeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myData\n",
    "chooseBestFeatureToSplit(myData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 빈도수가 많은 항목 찾기\n",
    "def majorityCnt(classList):\n",
    "    classCnt = {}\n",
    "    for v in classList:\n",
    "        if v not in classCnt.keys():\n",
    "            classCnt[v] = 0\n",
    "        classCnt[v] += 1\n",
    "    sortedClassCnt = sorted(classCnt.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sortedClassCnt[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTree(dataSet, labels):\n",
    "    classList = [e[-1] for e in dataSet]\n",
    "    # 같은 등급의 class 인 경우 멈춤. Leaf \n",
    "    if classList.count(classList[0]) == len(classList):\n",
    "        return classList[0]\n",
    "    # 속성이 더 없으면 가장 많은 수를 반환.\n",
    "    if len(dataSet[0]) == 1:\n",
    "        return majorityCnt(classList)\n",
    "#     print(labels)\n",
    "    bestFeat = chooseBestFeatureToSplit(dataSet)\n",
    "    bestFeatLabel = labels[bestFeat]\n",
    "    \n",
    "    myTree = {bestFeatLabel:{}} # 트리 구조 쌓기 위함.\n",
    "    del(labels[bestFeat]) # 사용된 속성은 삭제\n",
    "    \n",
    "    featValues = [ex[bestFeat] for ex in dataSet]\n",
    "    uniqueVals = set(featValues)\n",
    "    for value in uniqueVals:\n",
    "        subLabels = labels[:]\n",
    "        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels) # 재귀적으로 호출.\n",
    "    \n",
    "    return myTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}\n"
     ]
    }
   ],
   "source": [
    "myData, labels = createDataSet()\n",
    "myTree = createTree(myData, labels)\n",
    "print(myTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
